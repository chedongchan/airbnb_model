{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "\n",
    "from sklearn import datasets\n",
    "from sklearn.feature_selection import SelectKBest, f_regression\n",
    "from sklearn.metrics import r2_score,mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler,StandardScaler\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from sklearn.datasets import load_diabetes\n",
    "\n",
    "# df = pd.read_csv('tabular_data/tabular_data/clean_tabular_data.csv')\n",
    "# X = df[[\"guests\",\"beds\",\"bathrooms\",\"Cleanliness_rating\",\"Accuracy_rating\",\"Communication_rating\",\"Location_rating\",\"Check-in_rating\",\"Value_rating\",\"amenities_count\"]] # get all columns except the last one\n",
    "# y = df['Price_Night']  # get the 'Price_Night' column\n",
    "\n",
    "X,y = load_diabetes(return_X_y=True,as_frame=True)\n",
    "X = X.to_numpy()\n",
    "y = y.to_numpy()\n",
    "X= X.astype('float32')\n",
    "y = y.astype('float32')\n",
    "y = y.reshape(-1,1)\n",
    "\n",
    "X_scaler = MinMaxScaler()\n",
    "X_scaled = X_scaler.fit_transform(X)\n",
    "\n",
    "y_scaler = MinMaxScaler()\n",
    "y_scaled = y_scaler.fit_transform(y)\n",
    "y_scaled = y_scaled.ravel()\n",
    "\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X_scaled, y_scaled, test_size=0.33, random_state=42)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y_scaled, test_size=0.33, random_state=42)\n",
    "\n",
    "class TransformFn:\n",
    "    def __init__(self,k):\n",
    "        self.top_k_scores_list = []\n",
    "        self.top_k_names_list = []\n",
    "        self.k = k\n",
    "\n",
    "    def select_features(self, features, labels):\n",
    "        k = self.k\n",
    "        selector = SelectKBest(k=k, score_func=f_regression)\n",
    "        selected_features = selector.fit_transform(features, labels)\n",
    "        scores = selector.scores_\n",
    "        if self.k != 'all':\n",
    "            top_k_indices = np.argpartition(scores, -k)[-k:]\n",
    "            top_k_scores = scores[top_k_indices]\n",
    "            top_k_names = top_k_indices\n",
    "            self.top_k_scores_list = top_k_scores\n",
    "            self.top_k_names_list = top_k_names\n",
    "            \n",
    "        return selected_features, labels\n",
    "\n",
    "transform_fn1 = TransformFn(k=3)\n",
    "\n",
    "def to_tensor(sample):\n",
    "     return torch.from_numpy(sample)\n",
    "\n",
    "class Data(Dataset):\n",
    "\n",
    "  def __init__(self, X: np.ndarray, y: np.ndarray) -> None:\n",
    "\n",
    "    self.X = torch.from_numpy(X.astype(np.float32))\n",
    "    self.y = torch.from_numpy(y.astype(np.float32))\n",
    "    self.X = transform_fn1.select_features(X, y)[0]\n",
    "    self.y = transform_fn1.select_features(X, y)[1]\n",
    "    self.X = to_tensor(self.X)\n",
    "    self.y = to_tensor(self.y)\n",
    "\n",
    "  def __getitem__(self, index: int) -> tuple:\n",
    "\n",
    "    return self.X[index], self.y[index]\n",
    "\n",
    "  def __len__(self) -> int:\n",
    "    return self.X.shape[0]\n",
    "\n",
    "batch_size = 100\n",
    "\n",
    "# Generate the trainingand testing datasets\n",
    "train_dataset = Data(X_train, y_train)\n",
    "test_dataset = Data(X_test, y_test)\n",
    "\n",
    "train_data_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_data_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to run: 0.0 minutes\n",
      "\n",
      "Training MSE: 0.0\n",
      "Validation MSE: 0.0383\n",
      "\n",
      "Training r2: 1.0\n",
      "Validation r2: 0.3142\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "from xgboost import plot_importance\n",
    "xgb_reg_start = time.time()\n",
    "xgb_reg = xgb.XGBRegressor()\n",
    "xgb_reg.fit(X_train, y_train)\n",
    "training_preds_xgb_reg = xgb_reg.predict(X_train)\n",
    "val_preds_xgb_reg = xgb_reg.predict(X_test)\n",
    "\n",
    "\n",
    "xgb_reg_end = time.time()\n",
    "\n",
    "print(f\"Time taken to run: {round((xgb_reg_end - xgb_reg_start)/60,1)} minutes\")\n",
    "print(\"\\nTraining MSE:\", round(mean_squared_error(y_train, training_preds_xgb_reg),4))\n",
    "print(\"Validation MSE:\", round(mean_squared_error(y_test, val_preds_xgb_reg),4))\n",
    "print(\"\\nTraining r2:\", round(r2_score(y_train, training_preds_xgb_reg),4))\n",
    "print(\"Validation r2:\", round(r2_score(y_test, val_preds_xgb_reg),4))\n",
    "# ft_weights_xgb_reg = pd.DataFrame(xgb_reg.feature_importances_, columns=['weight'], index=X_train.columns)\n",
    "# ft_weights_xgb_reg.sort_values('weight', inplace=True)\n",
    "# ft_weights_xgb_reg\n",
    "# # Plotting feature importances\n",
    "# plt.figure(figsize=(8,20))\n",
    "# plt.barh(ft_weights_xgb_reg.index, ft_weights_xgb_reg.weight, align='center') \n",
    "# plt.title(\"Feature importances in the XGBoost model\", fontsize=14)\n",
    "# plt.xlabel(\"Feature importance\")\n",
    "# plt.margins(y=0.01)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "Epochs:10000 | Batches per epoch:   3 | Loss: 0.05 | R2: 0.06727693635864562\n",
      "Epochs:20000 | Batches per epoch:   3 | Loss: 0.05 | R2: 0.21877801285531662\n",
      "Epochs:30000 | Batches per epoch:   3 | Loss: 0.03 | R2: 0.3884077927212519\n",
      "Epochs:40000 | Batches per epoch:   3 | Loss: 0.03 | R2: 0.5070038533800001\n",
      "Epochs:50000 | Batches per epoch:   3 | Loss: 0.03 | R2: 0.5379181232749511\n",
      "Epochs:60000 | Batches per epoch:   3 | Loss: 0.03 | R2: 0.4690031344830261\n",
      "Epochs:70000 | Batches per epoch:   3 | Loss: 0.03 | R2: 0.3046447973632038\n",
      "Epochs:80000 | Batches per epoch:   3 | Loss: 0.03 | R2: 0.48097066781493036\n",
      "Epochs:90000 | Batches per epoch:   3 | Loss: 0.03 | R2: 0.41247239382154943\n",
      "Epochs:100000 | Batches per epoch:   3 | Loss: 0.03 | R2: 0.5162677394616744\n",
      "The r2 value is: 0.489\n",
      "MSE Loss: 0.03035\n",
      "RMSE Loss: 0.17420\n"
     ]
    }
   ],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "  '''Linear Regression Model'''\n",
    "\n",
    "  def __init__(self, input_dim: int, hidden_dim: int, output_dim: int) -> None:\n",
    "    '''The network has 4 layers\n",
    "         - input layer\n",
    "         - hidden layer\n",
    "         - hidden layer\n",
    "         - output layer'''\n",
    "    super(NeuralNetwork, self).__init__()\n",
    "    self.input_to_hidden = nn.Linear(input_dim, hidden_dim)\n",
    "    self.hidden_layer_1 = nn.Linear(hidden_dim, hidden_dim)\n",
    "    self.hidden_activation_layer = nn.LeakyReLU(0.3)\n",
    "    self.hidden_layer_2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "    self.hidden_to_output = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "  def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "    # no activation and no softmax at the end\n",
    "    x = self.input_to_hidden(x)\n",
    "    x = self.hidden_layer_1(x)\n",
    "    x = self.hidden_activation_layer(x)\n",
    "    x = self.hidden_layer_2(x)\n",
    "    x = self.hidden_to_output(x)\n",
    "    return x\n",
    "\n",
    "\n",
    "input_dim = transform_fn1.k\n",
    "output_dim = 1\n",
    "# number of hidden layers\n",
    "hidden_layers =3\n",
    "\n",
    "model = NeuralNetwork(input_dim, hidden_layers,output_dim)\n",
    "learning_rate = 0.0005\n",
    "\n",
    "print(input_dim)\n",
    "\n",
    "# criterion to computes the loss between input and target\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# optimizer that will be used to update weights and biases\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, weight_decay=0.005)\n",
    "\n",
    "# start training\n",
    "\n",
    "writer = SummaryWriter()\n",
    "\n",
    "epochs = 100000\n",
    "for epoch in range(epochs):\n",
    "  running_loss = 0.0\n",
    "  for i, (inputs, labels) in enumerate(train_data_loader):\n",
    "    inputs = inputs.view(-1, input_dim)\n",
    "    labels = labels.view(-1, output_dim)\n",
    "    # forward propagation\n",
    "    outputs = model(inputs)\n",
    "    # loss = criterion(outputs, labels)\n",
    "\n",
    "    loss = F.mse_loss(outputs,labels)\n",
    "    \n",
    "    # set optimizer to zero grad to remove previous epoch gradients\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # backward propagation\n",
    "    loss.backward()\n",
    "\n",
    "    # optimize\n",
    "    optimizer.step()\n",
    "\n",
    "\n",
    "    outputs_detached = outputs.detach().numpy()\n",
    "    labels_detached = labels.detach().numpy()\n",
    "    loss = loss.detach().numpy()\n",
    "\n",
    "    outputs_unscaled = y_scaler.inverse_transform(outputs_detached.reshape(-1, 1))\n",
    "    labels_unscaled = y_scaler.inverse_transform(labels_detached.reshape(-1, 1))\n",
    "    loss_unscaled = y_scaler.inverse_transform(loss.reshape(-1, 1))\n",
    "    \n",
    "    r2 = r2_score(labels_unscaled,outputs_unscaled)\n",
    "\n",
    "    running_loss += loss\n",
    "\n",
    "    # running_loss += loss.item()\n",
    "  writer.add_scalar('Loss/train', running_loss, epoch)\n",
    "  writer.add_scalar('R2/train', r2, epoch)\n",
    "\n",
    "  # display statistics\n",
    "  loss_cal = running_loss/ (i + 1)\n",
    "  loss_str= np.format_float_positional(loss_cal, precision=2)\n",
    "  if not ((epoch + 1) % (epochs // 10)):\n",
    "    print(f'Epochs:{epoch + 1:5d} | ' \\\n",
    "          f'Batches per epoch: {i + 1:3d} | ' \\\n",
    "          f'Loss: {loss_str} | ' \\\n",
    "          f'R2: {r2}')\n",
    "\n",
    "writer.close()\n",
    "# testdata = Data(X_test, y_test)\n",
    "# testloader = DataLoader(testdata, batch_size=batch_size, \n",
    "#                         shuffle=True)\n",
    "# Validate trained model using the test dataset\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "  loss = 0\n",
    "  for i, (inputs, labels) in enumerate(test_data_loader):\n",
    "    # calculate output by running through the network\n",
    "    inputs = inputs.view(-1, input_dim)\n",
    "    labels = labels.view(-1, output_dim)\n",
    "    predictions = model(inputs)\n",
    "\n",
    "    predictions_detached = predictions.detach().numpy()\n",
    "    labels_detached = labels.detach().numpy()\n",
    "\n",
    "    y_test_unscaled = y_scaler.inverse_transform(labels_detached.reshape(-1, 1))\n",
    "    predictions_unscaled = y_scaler.inverse_transform(predictions_detached.reshape(-1, 1))\n",
    "\n",
    "    loss += F.mse_loss(predictions, labels)\n",
    "    r2 = r2_score(y_test_unscaled,predictions_unscaled)\n",
    "  \n",
    "  mse = loss / (i + 1)\n",
    "  rmse = np.sqrt(mse)\n",
    "  print(f\"The r2 value is: {r2:.3f}\")\n",
    "  print(f'MSE Loss: {mse:.5f}')\n",
    "  print(f'RMSE Loss: {rmse:.5f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "datascience",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6f4fdd0f09dde6c9c78e95782e94bdf3b5d4fb9a974195a1d7f1631fdbfb686e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
