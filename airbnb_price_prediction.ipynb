{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import time\n",
    "import os\n",
    "import json\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "class PriceNightDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.tensor(X.values).float()\n",
    "        self.y = torch.tensor(y.values).float().view(-1, 1)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.X[index], self.y[index]\n",
    "\n",
    "def generate_nn_configs(num_configs):\n",
    "    \"\"\"Generate a list of config dictionaries for the neural network.\n",
    "\n",
    "    Parameters:\n",
    "    num_configs (int): The number of config dictionaries to generate.\n",
    "\n",
    "    Returns:\n",
    "    list: A list of config dictionaries.\n",
    "    \"\"\"\n",
    "    configs = []\n",
    "    for _ in range(num_configs):\n",
    "        config = {\n",
    "            'learning_rate': random.uniform(0.0001, 0.1),\n",
    "            'hidden_size': random.randint(10, 2000)\n",
    "        }\n",
    "        configs.append(config)\n",
    "    return configs\n",
    "\n",
    "def train(X_train, y_train, config, train_loader):\n",
    "    \"\"\"Train a neural network model.\n",
    "\n",
    "    Parameters:\n",
    "    X_train (pandas DataFrame): The training input data.\n",
    "    y_train (pandas Series): The training output data.\n",
    "    config (dict): A dictionary containing the hyperparameters for the model.\n",
    "    train_loader (torch DataLoader): A DataLoader for the training set.\n",
    "\n",
    "    Returns:\n",
    "    torch model: The trained model.\n",
    "    \"\"\"\n",
    "    # Define the model\n",
    "    model = torch.nn.Sequential(\n",
    "        torch.nn.Linear(10, config['hidden_size']),  # 10 input features, hidden_size outputs\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.Linear(config['hidden_size'], 1)  # hidden_size inputs, 1 output (prediction)\n",
    "    )\n",
    "\n",
    "    # Define the optimizer\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=config['learning_rate'])\n",
    "\n",
    "    # Define the loss function\n",
    "    loss_fn = torch.nn.MSELoss()\n",
    "    since = time.time()\n",
    "    # Train the model\n",
    "    for i in range(100):\n",
    "        # Initialize the loss for the epoch\n",
    "        epoch_loss = 0\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            # Make predictions and compute the loss\n",
    "            y_pred = model(X_batch)\n",
    "            print(X_batch.shape)\n",
    "            print(y_pred.shape)\n",
    "            print(y_batch.shape)\n",
    "            loss = loss_fn(y_pred, y_batch)\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "            # Backpropagate the gradients and update the model parameters\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        print(epoch_loss)\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def evaluate(model, X_test_tensor, y_test_tensor, test_loader):\n",
    "    \"\"\"Evaluate a model on the test set.\n",
    "\n",
    "    Parameters:\n",
    "    model (torch model): The model to evaluate.\n",
    "    X_test (pandas DataFrame): The test input data.\n",
    "    y_test (pandas Series): The test output data.\n",
    "    test_loader (torch DataLoader): A DataLoader for the test set.\n",
    "\n",
    "    Returns:\n",
    "    tuple: A tuple containing the model's loss and R^2 value on the test set.\n",
    "    \"\"\"\n",
    "    # Define the loss function\n",
    "    loss_fn = torch.nn.MSELoss()\n",
    "    \n",
    "    # Initialize the loss and R^2 values for the test set\n",
    "    test_loss = 0\n",
    "    test_r2 = 0\n",
    "\n",
    "    # Evaluate the model on the test set\n",
    "    y_test_pred = model(X_test_tensor)\n",
    "    test_loss = loss_fn(y_test_pred, y_test_tensor)\n",
    "\n",
    "    # Calculate the R^2 value\n",
    "    y_test_mean = y_test_tensor.mean()\n",
    "    numerator = ((y_test_tensor - y_test_pred) ** 2).sum()\n",
    "    denominator = ((y_test_tensor - y_test_mean) ** 2).sum()\n",
    "    r2 = 1 - (numerator / denominator)\n",
    "\n",
    "    return test_loss, r2\n",
    "\n",
    "\n",
    "def save_model(model, save_dir, hyperparameters):\n",
    "    \"\"\"Save a model and its hyperparameters to the specified folder.\n",
    "\n",
    "    Parameters:\n",
    "    model (torch model): The model to save.\n",
    "    save_dir (str): The directory to save the model in.\n",
    "    hyperparameters (dict): A dictionary containing the model's hyperparameters.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "\n",
    "    # Save the model\n",
    "    torch.save(model.state_dict(), os.path.join(save_dir, 'model.pt'))\n",
    "\n",
    "    # Save the hyperparameters\n",
    "    with open(os.path.join(save_dir, 'hyperparameters.json'), 'w') as f:\n",
    "        json.dump(hyperparameters, f)\n",
    "\n",
    "def find_best_nn(X_train, y_train, X_test, y_test, num_configs, save_dir):\n",
    "    \"\"\"Find the best neural network by training a model for each config.\n",
    "\n",
    "    Parameters:\n",
    "    X_train (pandas DataFrame): The training input data.\n",
    "    y_train (pandas Series): The training output data.\n",
    "    X_test (pandas DataFrame): The test input data.\n",
    "    y_test (pandas Series): The test output data.\n",
    "    num_configs (int): The number of configs to generate and evaluate.\n",
    "    save_dir (str): The directory to save the best model in.\n",
    "\n",
    "    Returns:\n",
    "    tuple: A tuple containing the best model, the metrics of the best model, and the hyperparameters of the best model.\n",
    "    \"\"\"\n",
    "    # Generate configs\n",
    "    configs = generate_nn_configs(num_configs)\n",
    "\n",
    "    # Initialize the best model and its metrics and hyperparameters\n",
    "    best_model = None\n",
    "    best_metrics = {'loss': float('inf'), 'r2': -float('inf')}\n",
    "    best_hyperparameters = None\n",
    "    best_config = None\n",
    "\n",
    "    # Convert the data to tensors\n",
    "    X_train_tensor = torch.tensor(X_train.values).float()\n",
    "    y_train_tensor = torch.tensor(y_train.values).float().view(-1, 1)  # reshape to be a 2D tensor\n",
    "    X_test_tensor = torch.tensor(X_test.values).float()\n",
    "    y_test_tensor = torch.tensor(y_test.values).float().view(-1, 1)  # reshape to be a 2D tensor\n",
    "\n",
    "    # Create datasets for the training and test sets\n",
    "    train_dataset = PriceNightDataset(X_train, y_train)\n",
    "    test_dataset = PriceNightDataset(X_test, y_test)\n",
    "\n",
    "    # Create DataLoaders for the training and test sets\n",
    "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)  \n",
    "\n",
    "    writer = SummaryWriter()\n",
    "\n",
    "    # Train and evaluate models for each config\n",
    "    for config in configs:\n",
    "        # Train the model\n",
    "        start_time = time.time()\n",
    "        model = train(X_train, y_train, config, train_loader)\n",
    "\n",
    "        # Evaluate the model on the test set\n",
    "        test_loss, r2 = evaluate(model, X_test_tensor, y_test_tensor, test_loader)\n",
    "\n",
    "        # Log the loss and R^2 values to TensorBoard\n",
    "        writer.add_scalar('loss', test_loss, global_step=config['hidden_size'])\n",
    "        writer.add_scalar('r2', r2, global_step=config['hidden_size'])\n",
    "\n",
    "        # Get time taken and inference latency \n",
    "        end_time = time.time()\n",
    "        latency = end_time - start_time\n",
    "\n",
    "        # Save the model if it performs the best so far\n",
    "        if test_loss < best_metrics['loss'] and r2 > best_metrics['r2']:\n",
    "            best_model = model\n",
    "            best_metrics = {'loss': test_loss, 'r2': r2}\n",
    "            best_hyperparameters = config\n",
    "            best_config=config\n",
    "            additional_info = {'time_taken': latency,\n",
    "                      'inference_latency': latency / len(X),  # inference latency per sample\n",
    "                      'r_squared': r2.item()}\n",
    "            print(best_hyperparameters)\n",
    "            best_hyperparameters.update(additional_info)\n",
    "            print(best_hyperparameters)\n",
    "            save_model(best_model, save_dir, best_hyperparameters)\n",
    "            print(best_hyperparameters)\n",
    "            print(best_metrics)\n",
    "\n",
    "\n",
    "    # Plot the loss and R^2 values\n",
    "    test_loss=test_loss.detach().numpy()\n",
    "    r2=r2.detach().numpy()\n",
    "    plt.plot(config, test_loss, label='Loss')\n",
    "    plt.plot(config, r2, label='R^2')\n",
    "\n",
    "    writer.close()\n",
    "    return best_model, best_metrics, best_config\n",
    "\n",
    "\n",
    "# Read the CSV file and extract the 'Price_Night' column\n",
    "df = pd.read_csv('tabular_data/tabular_data/clean_tabular_data.csv')\n",
    "X = df[[\"guests\",\"beds\",\"bathrooms\",\"Cleanliness_rating\",\"Accuracy_rating\",\"Communication_rating\",\"Location_rating\",\"Check-in_rating\",\"Value_rating\",\"amenities_count\"]] # get all columns except the last one\n",
    "y = df['Price_Night']  # get the 'Price_Night' column\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "num_configs = 16\n",
    "save_dir = \"C:/Users/dongc/Desktop/Code/python/AiCore/airbnb_model/models/neural_networks/\"\n",
    "find_best_nn(X_train, y_train, X_test, y_test, num_configs, save_dir)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "datascience",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.8 | packaged by conda-forge | (main, Nov 24 2022, 14:07:00) [MSC v.1916 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6f4fdd0f09dde6c9c78e95782e94bdf3b5d4fb9a974195a1d7f1631fdbfb686e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
